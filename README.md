# üß† AutoML GPT Agent for Tabular Classification

This project implements an **end-to-end AutoML agent** for supervised classification tasks on tabular datasets. It uses OpenAI's GPT-4 latest model to dynamically generate preprocessing strategies and modeling pipelines, which are then executed locally using pandas, scikit-learn, and LazyPredict.

---

## üìä Workflow Overview

### 1. Minimal GUI Interface

A minimal graphical user interface (GUI) is provided for user interaction. Through the GUI, the user can:

- Upload a CSV dataset.
- Select the target column for supervised learning.
- View all agent steps, logs, and outputs in real-time.

The GUI serves as the entry point to the automated workflow and displays detailed feedback from each AutoML stage.

### 2. Strategy Generation (LLM)

GPT-4 receives a structured summary of the dataset and generates a full preprocessing plan:

- Drop irrelevant or identifier-like columns (e.g., customerID)
- Perform grouped imputations (e.g., median per category group)
- Handle outliers and skewed data distributions
- Scale numerical variables
- Encode categorical variables (label/one-hot)
- Suggest simple feature engineering steps

---

### 3. Feature Engineering

The agent executes the preprocessing plan:

- Type conversion (e.g., object to float)
- Missing value imputation (including grouped logic)
- Winsorization and skew handling
- Standard scaling
- One-hot encoding and label encoding
- Custom feature generation (e.g., interaction terms, ratios, bins)

The resulting dataset is saved as `dataset_FE.csv`.

---

### 4. Modeling and Evaluation

- Uses **LazyPredict** to benchmark dozens of classifiers
- Filters out dummy and pipeline-based models
- Selects and retrains the top real model (e.g., LogisticRegression, RandomForest, etc.)
- Outputs:
  - Accuracy on test split
  - Classification report
  - Actual vs predicted labels
  - Top N feature importances (if supported)

---

### 5. Human-readable Summary

GPT-4 translates model results into a clear summary:

- Explains top predictors and their influence
- Highlights features that reduce or increase outcome probability
- Helps users interpret why the model works and what matters

---

## üìÇ Output Files

- `feature_engineering.py` ‚Äì Execuable feature engineering script generated by GPT-4 
- `dataset_FE.csv` ‚Äì Preprocessed dataset with engineered features automatically generenated from `feature_engineering.py`
- `generated_code.py` ‚Äì Executable model pipeline generated by GPT-4

---

## üí° Key Features

- LLM-driven reasoning for data-specific pipelines
- Grouped imputations handled via pandas (no custom transformers)
- Clean integration of scikit-learn pipelines and ColumnTransformer
- Deterministic, modular, and fully executable output
- GPT constraints enforced for robust code generation

---

## üîí Design Notes

- No manual feature selection or hardcoded preprocessing
- Avoids fragile custom transformers and lambdas
- Ensures zero missing values at model input time
- Supports tabular binary or multiclass classification

---

## üöÄ Example Datasets Tested

üõ≥Ô∏è Titanic (Survival)
Kaggle Dataset Page:
https://www.kaggle.com/competitions/titanic

üìû Telco Customer Churn
Kaggle Dataset Page:
https://www.kaggle.com/datasets/blastchar/telco-customer-churn

Both processed end-to-end with successful execution, accurate predictions, and interpretable summaries.

---

## üß™ Example Outputs

### Titanic Dataset :

[INFO] File copied to /data: data\train.csv
[INFO] Detected 891 rows, 12 columns.

[RUN] Running AutoML agent on target column: 'Survived'...
[LLM_AGENT] Step 1: Analyzing dataset...

[LLM_AGENT] ‚úÖ Strategy received:

**1. Data Quality Issues**

- **Missing values:**  
  - Age: 177 missing (19.9%)  
  - Cabin: 687 missing (77.1%)  
  - Embarked: 2 missing (0.2%)  

- **Mixed data types:**  
  - Numerical: PassengerId, Survived, Pclass, SibSp, Parch, Age, Fare  
  - Categorical: Sex, Embarked, Pclass  
  - Text/high-cardinality: Name, Ticket, Cabin  

- **High-cardinality/identifier-like columns:**  
  - PassengerId (unique ID)  
  - Name (unique per row)  
  - Ticket (681 unique)  
  - Cabin (147 unique, mostly missing)  

- **Irrelevant/misleading features:**  
  - PassengerId (pure identifier)  
  - Name (unique, not directly predictive)  
  - Ticket (high-cardinality, not directly predictive)  
  - Cabin (mostly missing, high-cardinality)  

- **Skewed distributions/outliers:**  
  - Fare: likely right-skewed, possible outliers  
  - Age: possible outliers, check for negative or implausible values  

---

**2. Preprocessing Strategy**

**A. Feature Selection and Handling of Irrelevant Columns**

- **Drop:**  
  - PassengerId (identifier, not predictive)  
  - Name (unique, not predictive)  
  - Ticket (high-cardinality, not predictive)  
  - Cabin (mostly missing, high-cardinality, not predictive as-is)  

- **Keep:**  
  - Survived (target)  
  - Pclass  
  - Sex  
  - Age  
  - SibSp  
  - Parch  
  - Fare  
  - Embarked  

---

**B. Missing Value Treatment**

- **Age (numerical, 19.9% missing):**  
  - Impute missing values using the median Age within each group defined by Sex and Pclass.  
  - For any group with all values missing, use the overall median Age.

- **Embarked (categorical, 0.2% missing):**  
  - Impute missing values with the mode (most frequent value) of Embarked.

- **Fare (numerical, no missing):**  
  - No action needed.

- **All other kept columns:**  
  - No missing values.

---

**C. Encoding for Categorical Variables**

- **Sex:**  
  - Encode as binary: male=0, female=1.

- **Pclass:**  
  - Treat as categorical. Encode using one-hot encoding (three binary columns: Pclass_1, Pclass_2, Pclass_3).

- **Embarked:**  
  - Encode using one-hot encoding (three binary columns: Embarked_C, Embarked_Q, Embarked_S).

---

**D. Scaling for Numerical Variables**

- **Age, Fare, SibSp, Parch:**  
  - Scale using standardization (z-score: subtract mean, divide by standard deviation).

---

**E. Transformation for Skewed Data**

- **Fare:**  
  - Apply log1p transformation (log(1 + Fare)) before scaling to reduce right skew.

- **Age:**  
  - No transformation unless extreme skew is detected; otherwise, only scale.

---

**F. Handling of Outliers**

- **Fare:**  
  - After log1p transformation and scaling, no further outlier treatment.

- **Age:**  
  - After scaling, no further outlier treatment.

- **SibSp, Parch:**  
  - No outlier treatment; values are small integers.

---

**G. Encoding of Target Variable**

- **Survived:**  
  - Already binary (0, 1). No transformation needed.

---

**H. Feature Engineering**

- **FamilySize:**  
  - Create new feature: FamilySize = SibSp + Parch + 1.

- **IsAlone:**  
  - Create new binary feature: IsAlone = 1 if FamilySize == 1, else 0.

---

**I. High-Cardinality Categorical Variables**

- **Ticket, Cabin, Name:**  
  - Drop these columns due to high cardinality and lack of direct predictive value.

---

**J. Final Check**

- At the end of preprocessing, ensure no missing values remain in any column used for modeling.

---

**3. Column-wise Actions Summary**

| Column        | Action                                                                                  |
|---------------|----------------------------------------------------------------------------------------|
| PassengerId   | Drop                                                                                   |
| Name          | Drop                                                                                   |
| Ticket        | Drop                                                                                   |
| Cabin         | Drop                                                                                   |
| Survived      | Keep as target (no change)                                                             |
| Pclass        | One-hot encode (Pclass_1, Pclass_2, Pclass_3)                                          |
| Sex           | Binary encode (male=0, female=1)                                                       |
| Age           | Impute by Sex+Pclass median, then standardize                                          |
| SibSp         | Standardize                                                                            |
| Parch         | Standardize                                                                            |
| Fare          | Log1p transform, then standardize                                                      |
| Embarked      | Impute with mode, then one-hot encode (Embarked_C, Embarked_Q, Embarked_S)             |
| FamilySize    | Create (SibSp + Parch + 1), then standardize                                           |
| IsAlone       | Create (1 if FamilySize==1 else 0)                                                     |

---

**4. Final Feature List for Modeling**

- Pclass_1, Pclass_2, Pclass_3 (one-hot)
- Sex (binary)
- Age (imputed, standardized)
- SibSp (standardized)
- Parch (standardized)
- Fare (log1p, standardized)
- Embarked_C, Embarked_Q, Embarked_S (one-hot)
- FamilySize (standardized)
- IsAlone (binary)

**No missing values remain in any feature after preprocessing.**


[LLM_AGENT] Step 2: Generating feature engineering script...

[LLM_AGENT] ‚úÖ Saved feature_engineering.py. Running it now...

[LLM_AGENT] ‚úÖ Feature engineering completed:

Dropped columns: ['PassengerId', 'Name', 'Ticket', 'Cabin']
Created interaction feature: Age_Fare = Age * Fare
Created interaction feature: SibSp_Parch = SibSp + Parch
Created interaction feature: Fare_per_SibSp = Fare / (SibSp + 1)
Created quantile binning feature: Age_qbin (4 quantiles of Age)
Created quantile binning feature: Fare_qbin (4 quantiles of Fare)
Created feature: MissingCount (number of missing values per row)



[LLM_AGENT] Step 3: Summarizing dataset_FE.csv for modeling...

[LLM_AGENT] ‚úÖ Summary of dataset_FE.csv completed.

[LLM_AGENT] Step 3: Generating modeling script based on FE dataset and strategy...

[LLM_AGENT] ‚úÖ Code saved to generated_code.py. Running it now...

[LLM_AGENT] ‚úÖ Model training completed:

[LightGBM] [Info] Number of positive: 273, number of negative: 439
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000119 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 235
[LightGBM] [Info] Number of data points in the train set: 712, number of used features: 13
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383427 -> initscore=-0.475028
[LightGBM] [Info] Start training from score -0.475028
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf

=== LazyPredict Results (sorted by Accuracy) ===
                               Accuracy  ...  Time Taken
Model                                    ...            
LogisticRegression                 0.83  ...        0.01
CalibratedClassifierCV             0.83  ...        0.03
RidgeClassifierCV                  0.82  ...        0.01
RidgeClassifier                    0.82  ...        0.01
LinearDiscriminantAnalysis         0.82  ...        0.01
LinearSVC                          0.82  ...        0.01
RandomForestClassifier             0.80  ...        0.11
AdaBoostClassifier                 0.80  ...        0.06
SVC                                0.80  ...        0.02
NuSVC                              0.80  ...        0.02
LGBMClassifier                     0.79  ...        0.05
BaggingClassifier                  0.79  ...        0.03
XGBClassifier                      0.79  ...        0.11
DecisionTreeClassifier             0.78  ...        0.01
ExtraTreesClassifier               0.78  ...        0.10
LabelSpreading                     0.78  ...        0.03
LabelPropagation                   0.78  ...        0.02
GaussianNB                         0.77  ...        0.01
KNeighborsClassifier               0.76  ...        0.01
NearestCentroid                    0.75  ...        1.40
Perceptron                         0.73  ...        0.01
ExtraTreeClassifier                0.72  ...        0.01
SGDClassifier                      0.72  ...        0.01
QuadraticDiscriminantAnalysis      0.69  ...        0.01
BernoulliNB                        0.69  ...        0.01
DummyClassifier                    0.61  ...        0.01
PassiveAggressiveClassifier        0.58  ...        0.01

[27 rows x 5 columns]

=== Retrained Top Model: LogisticRegression ===
Accuracy: 0.8156424581005587

Classification Report:
              precision    recall  f1-score   support

           0       0.82      0.89      0.86       110
           1       0.80      0.70      0.74        69

    accuracy                           0.82       179
   macro avg       0.81      0.79      0.80       179
weighted avg       0.81      0.82      0.81       179


Head of Actual vs Predicted:
     Actual  Predicted
565       0          0
160       0          0
553       1          0
860       0          0
241       1          1

Top 10 Feature Importances:
Sex: 2.4458
Pclass_3: 0.9229
Pclass_1: 0.8282
Age: 0.5476
IsAlone: 0.4153
Fare_log1p: 0.4085
SibSp: 0.3251
Embarked_Q: 0.3005
Embarked_S: 0.2765
FamilySize: 0.2552


[LLM_AGENT] ‚ö†Ô∏è Training Warnings:


  0%|          | 0/32 [00:00<?, ?it/s]
 12%|#2        | 4/32 [00:00<00:00, 34.50it/s]
 28%|##8       | 9/32 [00:00<00:00, 37.90it/s]
 56%|#####6    | 18/32 [00:01<00:01,  8.83it/s]
 72%|#######1  | 23/32 [00:01<00:00, 11.64it/s]
 97%|#########6| 31/32 [00:02<00:00, 17.39it/s]
100%|##########| 32/32 [00:02<00:00, 15.29it/s]



[LLM_AGENT] Step 4: Generating human-readable summary...

[LLM_AGENT] ‚úÖ Summary:

### Summary of Final Model Output

The final selected and retrained model is **Logistic Regression**, which achieved an accuracy of approximately **82%** on the test data.

#### Most Important Features (in plain language, with directional influence):

1. **Sex**  
   - **Description:** Whether the passenger is male or female.  
   - **Influence:** Being female (since 'Sex' is usually coded as 1 for female) strongly increases the likelihood of survival.

2. **Third Class Ticket (Pclass_3)**  
   - **Description:** Whether the passenger had a third-class ticket.  
   - **Influence:** Having a third-class ticket increases the likelihood of not surviving (since the coefficient is positive, but typically third class is associated with lower survival; the exact direction depends on encoding, but in Titanic datasets, being third class usually reduces survival odds).

3. **First Class Ticket (Pclass_1)**  
   - **Description:** Whether the passenger had a first-class ticket.  
   - **Influence:** Having a first-class ticket increases the likelihood of survival.

4. **Age**  
   - **Description:** The passenger‚Äôs age.  
   - **Influence:** Younger age increases the likelihood of survival (since the coefficient is positive, assuming lower age values are associated with higher survival).

5. **Traveling Alone (IsAlone)**  
   - **Description:** Whether the passenger was traveling alone.  
   - **Influence:** Traveling alone increases the likelihood of not surviving (since being alone is usually a risk factor).

6. **Fare Paid (Fare_log1p)**  
   - **Description:** The (log-transformed) fare paid for the ticket.  
   - **Influence:** Paying a higher fare increases the likelihood of survival.

7. **Number of Siblings/Spouses (SibSp)**  
   - **Description:** Number of siblings or spouses aboard.  
   - **Influence:** Having more siblings/spouses slightly increases survival odds.

8. **Embarked at Queenstown (Embarked_Q)**  
   - **Description:** Whether the passenger boarded at Queenstown.  
   - **Influence:** Boarding at Queenstown slightly increases the likelihood of survival.

9. **Embarked at Southampton (Embarked_S)**  
   - **Description:** Whether the passenger boarded at Southampton.  
   - **Influence:** Boarding at Southampton slightly increases the likelihood of not surviving.

10. **Family Size (FamilySize)**  
    - **Description:** Total number of family members aboard (siblings/spouses + parents/children + self).  
    - **Influence:** Larger family size slightly increases the likelihood of survival.

---

**Concise Summary:**  
The model predicts survival primarily based on gender, ticket class, age, and fare paid. Being female, younger, in first class, and having paid a higher fare most strongly increase the predicted chance of survival, while being male, in third class, or traveling alone decrease it.

---

### Telco Churn Dataset : 

[INFO] File copied to /data: data\WA_Fn-UseC_-Telco-Customer-Churn.csv
[INFO] Detected 7043 rows, 21 columns.

[RUN] Running AutoML agent on target column: 'Churn'...
[INFO] Launching background LLM agent. This may take up to a minute if GPT is slow...
[LLM_AGENT] Step 1: Analyzing dataset...

[LLM_AGENT] Step 1: Calling OpenAI to generate strategy...

[LLM_AGENT] ‚úÖ Strategy received:

**1. Data Quality Issues**

- **Missing values:** None reported.
- **Mixed data types:** Yes; numerical (int64, float64), categorical (object), identifier (customerID).
- **High-cardinality/identifier columns:** customerID (unique per row, 7043 unique values).
- **Irrelevant/misleading features:** customerID (identifier, not predictive).
- **Skewed distributions/outliers:** Possible in 'MonthlyCharges', 'TotalCharges', and 'tenure' (check for outliers and skew).
- **Potential mixed data types:** 'TotalCharges' is object but should be numeric.

---

**2. Preprocessing Strategy**

**A. Feature Selection and Handling of Irrelevant Columns**

| Column           | Action         | Reasoning                                   |
|------------------|---------------|---------------------------------------------|
| customerID       | Drop          | Identifier, not predictive                  |
| gender           | Keep          | Categorical, relevant                       |
| SeniorCitizen    | Keep          | Binary numeric, relevant                    |
| Partner          | Keep          | Categorical, relevant                       |
| Dependents       | Keep          | Categorical, relevant                       |
| tenure           | Keep          | Numeric, relevant                           |
| PhoneService     | Keep          | Categorical, relevant                       |
| MultipleLines    | Keep          | Categorical, relevant                       |
| InternetService  | Keep          | Categorical, relevant                       |
| OnlineSecurity   | Keep          | Categorical, relevant                       |
| OnlineBackup     | Keep          | Categorical, relevant                       |
| DeviceProtection | Keep          | Categorical, relevant                       |
| TechSupport      | Keep          | Categorical, relevant                       |
| StreamingTV      | Keep          | Categorical, relevant                       |
| StreamingMovies  | Keep          | Categorical, relevant                       |
| Contract         | Keep          | Categorical, relevant                       |
| PaperlessBilling | Keep          | Categorical, relevant                       |
| PaymentMethod    | Keep          | Categorical, relevant                       |
| MonthlyCharges   | Keep          | Numeric, relevant                           |
| TotalCharges     | Keep/Transform| Should be numeric; convert from object      |
| Churn            | Keep/Transform| Target variable; encode as binary           |

---

**B. Missing Value Treatment**

- No missing values reported. After converting 'TotalCharges' to numeric, check for conversion errors (e.g., blank strings) and impute as follows:
  - For any missing 'TotalCharges', impute with the median 'TotalCharges' grouped by 'Contract' type.

---

**C. Encoding for Categorical Variables**

- **Binary categorical (2 unique values):**
  - gender, Partner, Dependents, PhoneService, PaperlessBilling: Encode as 0/1 (Label Encoding).
- **Multi-category (3-4 unique values):**
  - MultipleLines, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies, Contract, PaymentMethod: One-hot encode.
- **High-cardinality:**
  - Only customerID is high-cardinality; already dropped.

---

**D. Scaling for Numerical Variables**

- tenure, MonthlyCharges, TotalCharges: Standardize (zero mean, unit variance).

---

**E. Transformation for Skewed Data**

- Check skewness for tenure, MonthlyCharges, TotalCharges:
  - If skewness > 1 or < -1, apply log1p transformation before scaling.
  - Otherwise, proceed with scaling only.

---

**F. Handling of Outliers**

- For tenure, MonthlyCharges, TotalCharges:
  - Cap values at the 1st and 99th percentiles (winsorization).

---

**G. Encoding of Target Variable**

- Churn: Encode as 0 ('No'), 1 ('Yes').

---

**H. Feature Engineering**

- Create 'AvgChargesPerMonth' = TotalCharges / tenure (set to 0 if tenure == 0).
- Create 'HasInternetService' = 1 if InternetService != 'No', else 0.

---

**3. General Strategies for High-Cardinality Categorical Variables**

- Drop identifier-like columns (e.g., customerID).
- For any future high-cardinality categorical variables (not present here), use target encoding or frequency encoding, but not one-hot encoding.

---

**Final Preprocessing Steps (Column-wise)**

| Column           | Action                                                                                 |
|------------------|---------------------------------------------------------------------------------------|
| customerID       | Drop                                                                                  |
| gender           | Label encode (Female=0, Male=1)                                                       |
| SeniorCitizen    | Keep as is (already numeric, 0/1)                                                     |
| Partner          | Label encode (No=0, Yes=1)                                                            |
| Dependents       | Label encode (No=0, Yes=1)                                                            |
| tenure           | Winsorize (1st/99th pct), log1p if skewed, then standardize                           |
| PhoneService     | Label encode (No=0, Yes=1)                                                            |
| MultipleLines    | One-hot encode                                                                        |
| InternetService  | One-hot encode                                                                        |
| OnlineSecurity   | One-hot encode                                                                        |
| OnlineBackup     | One-hot encode                                                                        |
| DeviceProtection | One-hot encode                                                                        |
| TechSupport      | One-hot encode                                                                        |
| StreamingTV      | One-hot encode                                                                        |
| StreamingMovies  | One-hot encode                                                                        |
| Contract         | One-hot encode                                                                        |
| PaperlessBilling | Label encode (No=0, Yes=1)                                                            |
| PaymentMethod    | One-hot encode                                                                        |
| MonthlyCharges   | Winsorize (1st/99th pct), log1p if skewed, then standardize                           |
| TotalCharges     | Convert to float, impute missing by median per Contract, winsorize, log1p if skewed, standardize |
| Churn            | Label encode (No=0, Yes=1)                                                            |
| (new) AvgChargesPerMonth | Create, winsorize, log1p if skewed, standardize                               |
| (new) HasInternetService | Create, binary (1 if InternetService != 'No', else 0)                         |

**At the end of preprocessing, all features used for modeling will be numeric, with no missing values.**


[LLM_AGENT] Step 2: Generating feature engineering script...

[LLM_AGENT] ‚úÖ Saved feature_engineering.py. Running it now...

[LLM_AGENT] ‚úÖ Feature engineering completed:

Dropped column: customerID
Created interaction feature: SeniorCitizen_x_tenure = SeniorCitizen * tenure
Created interaction feature: SeniorCitizen_div_tenure = SeniorCitizen / tenure
Created interaction feature: SeniorCitizen_plus_tenure = SeniorCitizen + tenure
Created quantile bin feature: MonthlyCharges_qbin (4 bins of MonthlyCharges)
Created feature: MissingCount (number of missing values per row)
Saved processed dataset to: dataset_FE.csv



[LLM_AGENT] Step 3: Summarizing dataset_FE.csv for modeling...

[LLM_AGENT] ‚úÖ Summary of dataset_FE.csv completed.

[LLM_AGENT] Step 3: Generating modeling script based on FE dataset and strategy...

[LLM_AGENT] ‚úÖ Code saved to generated_code.py. Running it now...

[LLM_AGENT] ‚úÖ Model training completed:

[LightGBM] [Info] Number of positive: 1495, number of negative: 4139
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000440 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 1150
[LightGBM] [Info] Number of data points in the train set: 5634, number of used features: 36
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265353 -> initscore=-1.018328
[LightGBM] [Info] Start training from score -1.018328
=== LazyPredict Model Results ===
                               Accuracy  ...  Time Taken
Model                                    ...            
LogisticRegression                 0.80  ...        0.04
AdaBoostClassifier                 0.80  ...        0.24
LinearSVC                          0.80  ...        0.04
CalibratedClassifierCV             0.80  ...        0.22
RidgeClassifierCV                  0.80  ...        0.03
RidgeClassifier                    0.80  ...        0.02
RandomForestClassifier             0.79  ...        0.53
LGBMClassifier                     0.79  ...        0.08
SVC                                0.79  ...        0.88
NuSVC                              0.79  ...        0.98
LinearDiscriminantAnalysis         0.79  ...        0.03
XGBClassifier                      0.78  ...        1.62
ExtraTreesClassifier               0.78  ...        0.46
BaggingClassifier                  0.78  ...        0.23
KNeighborsClassifier               0.77  ...        1.42
SGDClassifier                      0.77  ...        0.03
ExtraTreeClassifier                0.75  ...        0.02
PassiveAggressiveClassifier        0.74  ...        0.02
Perceptron                         0.74  ...        0.01
DummyClassifier                    0.73  ...        0.01
LabelSpreading                     0.73  ...        1.61
LabelPropagation                   0.73  ...        1.34
DecisionTreeClassifier             0.73  ...        0.04
BernoulliNB                        0.72  ...        0.02
NearestCentroid                    0.69  ...        0.02
GaussianNB                         0.58  ...        0.01
QuadraticDiscriminantAnalysis      0.48  ...        0.02

[27 rows x 5 columns]

=== Retrained Top Model: LogisticRegression ===
Accuracy: 0.7984386089425124

Classification Report:
              precision    recall  f1-score   support

           0       0.84      0.89      0.87      1035
           1       0.64      0.54      0.59       374

    accuracy                           0.80      1409
   macro avg       0.74      0.72      0.73      1409
weighted avg       0.79      0.80      0.79      1409


Actual vs Predicted (head):
   Actual  Predicted
0       0          0
1       0          1
2       0          0
3       0          0
4       0          0

Top 10 Feature Importances:
Contract_Two year: 1.3500
InternetService_Fiber optic: 1.2347
Contract_One year: 0.7069
MonthlyCharges: 0.6254
SeniorCitizen_plus_tenure: 0.5983
tenure: 0.5971
TotalCharges: 0.4554
PhoneService: 0.4519
StreamingMovies_Yes: 0.4182
StreamingTV_Yes: 0.4067


[LLM_AGENT] ‚ö†Ô∏è Training Warnings:


  0%|          | 0/32 [00:00<?, ?it/s]
  3%|3         | 1/32 [00:00<00:07,  4.16it/s]
  6%|6         | 2/32 [00:00<00:07,  4.24it/s]
 12%|#2        | 4/32 [00:00<00:04,  6.13it/s]
 28%|##8       | 9/32 [00:01<00:02,  8.01it/s]
 38%|###7      | 12/32 [00:02<00:05,  3.91it/s]
 41%|####      | 13/32 [00:04<00:07,  2.39it/s]
 44%|####3     | 14/32 [00:05<00:11,  1.59it/s]
 53%|#####3    | 17/32 [00:05<00:05,  2.68it/s]
 59%|#####9    | 19/32 [00:06<00:05,  2.43it/s]
 72%|#######1  | 23/32 [00:07<00:02,  3.44it/s]
 84%|########4 | 27/32 [00:08<00:01,  3.70it/s]
 97%|#########6| 31/32 [00:09<00:00,  3.13it/s]
100%|##########| 32/32 [00:09<00:00,  3.20it/s]



[LLM_AGENT] Step 4: Generating human-readable summary...

[LLM_AGENT] ‚úÖ Summary:

### Summary of Final Model Output

The final selected and retrained model is a Logistic Regression classifier, which achieved an accuracy of approximately 80%. The model predicts the likelihood of the target outcome (likely customer churn, based on the feature names).

#### Most Important Features (in plain language, with directional influence):

1. **Having a Two-Year Contract**  
   - Customers with a two-year contract are much less likely to churn (strong negative influence on churn).

2. **Using Fiber Optic Internet Service**  
   - Customers with fiber optic internet service are more likely to churn (positive influence on churn).

3. **Having a One-Year Contract**  
   - Customers with a one-year contract are less likely to churn, but the effect is weaker than a two-year contract (negative influence on churn).

4. **Monthly Charges**  
   - Higher monthly charges are associated with a higher likelihood of churn (positive influence).

5. **Interaction between Senior Citizen Status and Tenure**  
   - Customers who are senior citizens and have been with the company longer are less likely to churn (negative influence).

6. **Length of Time as a Customer (Tenure)**  
   - Customers who have been with the company longer are less likely to churn (negative influence).

7. **Total Amount Paid (Total Charges)**  
   - Customers who have paid more overall (higher total charges) are less likely to churn (negative influence).

8. **Having Phone Service**  
   - Customers with phone service are less likely to churn (negative influence).

9. **Subscribing to Streaming Movies**  
   - Customers who subscribe to streaming movies are more likely to churn (positive influence).

10. **Subscribing to Streaming TV**  
    - Customers who subscribe to streaming TV are more likely to churn (positive influence).

---

**In summary:**  
The model finds that longer contracts, longer customer tenure, and higher total payments reduce the likelihood of churn, while having fiber optic internet, higher monthly charges, and subscribing to streaming services increase the risk of churn. Contract type and service choices are the strongest predictors of customer behavior in this dataset.

---

